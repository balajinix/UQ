{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEBOOK FOR PLAYING WITH UQ\n",
    "\n",
    "This is a notebook to explore UQ related work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datetime import date\n",
    "#import itertools\n",
    "#import os\n",
    "#import pickle\n",
    "#import numpy as np\n",
    "\n",
    "#import jsonlines\n",
    "#import pandas as pd\n",
    "#import sys\n",
    "\n",
    "#import json\n",
    "#import warnings\n",
    "#import requests\n",
    "\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "import sampling\n",
    "import numpy as np\n",
    "\n",
    "#from datasets import load_dataset\n",
    "#from evaluate import load\n",
    "#exact_match_metric = load(\"exact_match\")\n",
    "\n",
    "import evaluation\n",
    "import uq_from_similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# MatPlot inline magic and plot prettification\n",
    "#%matplotlib inline\n",
    "#matplotlib.style.use('seaborn-whitegrid')\n",
    "#font = {'size'   : 22}\n",
    "#matplotlib.rc('font', **font)\n",
    "\n",
    "# Widen the notebook...\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "#pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = 'spider_codellama'\n",
    "\n",
    "#dataset = 'spider_deepseeker'\n",
    "\n",
    "#dataset = 'spider_granite'\n",
    "\n",
    "dataset = 'spider_realistic_codellama'\n",
    "\n",
    "sampling_type = 'temp_first'\n",
    "#sampling_type = 'temp_all'\n",
    "split = 'dev'\n",
    "#split = 'test'\n",
    "temperature = None\n",
    "\n",
    "#sampling_type = 'standard'\n",
    "#temperature = '0.25'\n",
    "\n",
    "#temperature = '1.0'\n",
    "\n",
    "\n",
    "#dataset = 'spider_fine-tuned_granite'\n",
    "# the following options don't matter for this dataset\n",
    "#sampling_type = 'temp_all'\n",
    "#split = 'dev'\n",
    "#temperature = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "num_test_samples = 5\n",
    "frac_test = 0.5\n",
    "num_bins = 5\n",
    "#num_bins = 10\n",
    "\n",
    "restrict_sample_indices_bool = False\n",
    "accepted_sample_indices = None\n",
    "\n",
    "#restrict_sample_indices_bool = True\n",
    "#accepted_sample_indices = range(0,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of queries: 508\n",
      "setting: gens|output_type|agg-arith\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.6311434955973039 +/- 0.026172088452306286\n",
      "AUARC:  0.5247591600650277 +/- 0.006720859395355827\n",
      "ECE:  0.3288713910761157 +/- 0.011154855643044692\n",
      "ACE:  0.31557705262179 +/- 0.021564972162061075\n",
      "RMSCE:  0.3612854767402863 +/- 0.010094735384895942\n",
      "\n",
      "\n",
      "setting: gens|jaccard|agg-arith\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.7686352438992468 +/- 0.024377448258887935\n",
      "AUARC:  0.6355386216332217 +/- 0.021118291351160223\n",
      "ECE:  0.055663252204700545 +/- 0.014069712170764352\n",
      "ACE:  0.07988794729148829 +/- 0.006752848272280489\n",
      "RMSCE:  0.08005272778555884 +/- 0.006275542987019961\n",
      "\n",
      "\n",
      "setting: gens|rouge1|agg-arith\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.7509306112655849 +/- 0.02562046136803786\n",
      "AUARC:  0.6156994140206629 +/- 0.02255871823929534\n",
      "ECE:  0.11749647100401481 +/- 0.00578323473409284\n",
      "ACE:  0.19609223012561322 +/- 0.013957316393671648\n",
      "RMSCE:  0.13352265186635334 +/- 0.00665653162769092\n",
      "\n",
      "\n",
      "setting: gens|rougeL|agg-arith\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.7651072320142757 +/- 0.025229498529241445\n",
      "AUARC:  0.6238818280541059 +/- 0.021906131736751433\n",
      "ECE:  0.09906980337175056 +/- 0.007857456282631868\n",
      "ACE:  0.18023954227553565 +/- 0.012729805729448027\n",
      "RMSCE:  0.11611336647634234 +/- 0.009065920837525769\n",
      "\n",
      "\n",
      "setting: gens|output_type|bayes-post:beta\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.6314530771555528 +/- 0.022066941227515235\n",
      "AUARC:  0.5326351855194028 +/- 0.011276781742857123\n",
      "ECE:  0.3055088090794641 +/- 0.05528969162588718\n",
      "ACE:  0.3064652842264119 +/- 0.052898503758515444\n",
      "RMSCE:  0.3298810756716909 +/- 0.047384134297570324\n",
      "\n",
      "\n",
      "setting: gens|jaccard|bayes-post:beta\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.7668121693358656 +/- 0.02714522846734102\n",
      "AUARC:  0.6390159185496758 +/- 0.020949961000144512\n",
      "ECE:  0.14759952174412844 +/- 0.029514262345862456\n",
      "ACE:  0.14631817766808994 +/- 0.02763977432058133\n",
      "RMSCE:  0.1586366187011597 +/- 0.028403638402382733\n",
      "\n",
      "\n",
      "setting: gens|rouge1|bayes-post:beta\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.7989030330211286 +/- 0.018989622584402843\n",
      "AUARC:  0.6482895903184315 +/- 0.021127118502972997\n",
      "ECE:  0.13744958415232159 +/- 0.02800211589124027\n",
      "ACE:  0.1321143980157582 +/- 0.02404573151285075\n",
      "RMSCE:  0.14934638642222864 +/- 0.030517812114598857\n",
      "\n",
      "\n",
      "setting: gens|rougeL|bayes-post:beta\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.8094042974650121 +/- 0.01884725590520181\n",
      "AUARC:  0.6539016727837348 +/- 0.02385304436469654\n",
      "ECE:  0.13816702530380903 +/- 0.025500157528419366\n",
      "ACE:  0.1364523987560699 +/- 0.02361016606482299\n",
      "RMSCE:  0.15008063419011744 +/- 0.02625261535542063\n",
      "\n",
      "\n",
      "setting: gens|output_type|spec-ecc\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.36801414880343003 +/- 0.02611204852478169\n",
      "AUARC:  0.36499200607972787 +/- 0.02480508897604264\n",
      "ECE:  0.3982163911790977 +/- 0.01084502719575653\n",
      "ACE:  0.3841814705091821 +/- 0.008773929234951294\n",
      "RMSCE:  0.45442358012228584 +/- 0.011349208341339484\n",
      "\n",
      "\n",
      "setting: gens|jaccard|spec-ecc\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.2694344528776631 +/- 0.02062293738437984\n",
      "AUARC:  0.32767537040387246 +/- 0.019932971582300202\n",
      "ECE:  0.5300957699969772 +/- 0.01606377007003701\n",
      "ACE:  0.5147177951715641 +/- 0.016188712447283066\n",
      "RMSCE:  0.5633813891109216 +/- 0.016584177026926772\n",
      "\n",
      "\n",
      "setting: gens|rouge1|spec-ecc\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.2698730961121534 +/- 0.024679845740950335\n",
      "AUARC:  0.31027880529650387 +/- 0.018744672308083327\n",
      "ECE:  0.4763881226036317 +/- 0.0168504528776795\n",
      "ACE:  0.4715910080398887 +/- 0.01960455762970889\n",
      "RMSCE:  0.5352983583021872 +/- 0.014240210357469363\n",
      "\n",
      "\n",
      "setting: gens|rougeL|spec-ecc\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.24972269494256535 +/- 0.018293826446176956\n",
      "AUARC:  0.2996326239079557 +/- 0.013778104950631809\n",
      "ECE:  0.4928974935745664 +/- 0.02085404389613335\n",
      "ACE:  0.49067471498635673 +/- 0.02026754052156926\n",
      "RMSCE:  0.5527098288430041 +/- 0.015227692298816764\n",
      "\n",
      "\n",
      "setting: gens|output_type|clf:lr\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.6302984754439578 +/- 0.02828463883567156\n",
      "AUARC:  0.5350129828645513 +/- 0.021959399040604177\n",
      "ECE:  0.07159357582100548 +/- 0.015268285956757926\n",
      "ACE:  0.12799163292384347 +/- 0.010753442132023026\n",
      "RMSCE:  0.133703156866423 +/- 0.012419567005668763\n",
      "\n",
      "\n",
      "setting: gens|jaccard|clf:lr\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.7546216793236111 +/- 0.029613782043744152\n",
      "AUARC:  0.6297592039048541 +/- 0.018771090384907985\n",
      "ECE:  0.06237078505357115 +/- 0.016734019434770406\n",
      "ACE:  0.08437766249287992 +/- 0.006895444399130798\n",
      "RMSCE:  0.08633257679455252 +/- 0.014674454944381334\n",
      "\n",
      "\n",
      "setting: gens|rouge1|clf:lr\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.8004029557907579 +/- 0.012606631405847557\n",
      "AUARC:  0.6515348283818215 +/- 0.020796653540146037\n",
      "ECE:  0.051051733861534644 +/- 0.006153427382947354\n",
      "ACE:  0.06930289368531303 +/- 0.008493137710463217\n",
      "RMSCE:  0.07364864838288565 +/- 0.011448276714986355\n",
      "\n",
      "\n",
      "setting: gens|rougeL|clf:lr\n",
      "# of test instances: 254\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.8085699905827092 +/- 0.010879473197458356\n",
      "AUARC:  0.6575053059212681 +/- 0.02114625667172243\n",
      "ECE:  0.05144256324326461 +/- 0.007045096780127148\n",
      "ACE:  0.06981926697888527 +/- 0.006694759532243272\n",
      "RMSCE:  0.07484414178722498 +/- 0.0093887276125421\n",
      "\n",
      "\n",
      "setting: gens|output_type|clf:rf\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.6275619260719918 +/- 0.021443265405756695\n",
      "AUARC:  0.521865656388978 +/- 0.007291520162576948\n",
      "ECE:  0.07117727911088009 +/- 0.014953897993116024\n",
      "ACE:  0.12649776846187633 +/- 0.013281847131777387\n",
      "RMSCE:  0.133556516696761 +/- 0.012303627690562452\n",
      "\n",
      "\n",
      "setting: gens|jaccard|clf:rf\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.7736202992476298 +/- 0.016307417911230804\n",
      "AUARC:  0.6361531231816191 +/- 0.021164240003607948\n",
      "ECE:  0.03771854365194254 +/- 0.0048858886266345755\n",
      "ACE:  0.05314129462799797 +/- 0.010852186878720735\n",
      "RMSCE:  0.056713023881775515 +/- 0.008147687733262212\n",
      "\n",
      "\n",
      "setting: gens|rouge1|clf:rf\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.7952501520960316 +/- 0.01068392974284832\n",
      "AUARC:  0.6406281784078965 +/- 0.008792441151397656\n",
      "ECE:  0.04683615496413433 +/- 0.01074881983332289\n",
      "ACE:  0.055094728295541254 +/- 0.016752452250574746\n",
      "RMSCE:  0.05873177603106135 +/- 0.016381451741159386\n",
      "\n",
      "\n",
      "setting: gens|rougeL|clf:rf\n",
      "# of test instances: 254\n",
      "\n",
      "\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Computed confidences up to query: 100\n",
      "Computed confidences up to query: 200\n",
      "Exec_acc:  0.455249343832021 +/- 0.011154855643044609\n",
      "AUROC:  0.8045674501189254 +/- 0.014104092536901869\n",
      "AUARC:  0.6473578595961765 +/- 0.010174870611679432\n",
      "ECE:  0.038601280246311334 +/- 0.010008967208386588\n",
      "ACE:  0.050351322052292556 +/- 0.008109932234438452\n",
      "RMSCE:  0.0576712441979295 +/- 0.012989162707655832\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run experiment\n",
    "\n",
    "evaluation.run_temp_first_exps(dataset, sampling_type, split, temperature, num_test_samples, frac_test, num_bins, \n",
    "                               restrict_sample_indices_bool, accepted_sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study Distribution of Similarity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset = 'spider_codellama'\n",
    "\n",
    "sampling_type = 'temp_first'\n",
    "#sampling_type = 'temp_all'\n",
    "split = 'dev'\n",
    "#split = 'test'\n",
    "temperature = None\n",
    "\n",
    "#sampling_type = 'standard'\n",
    "#temperature = '0.25'\n",
    "#temperature = '1.0'\n",
    "\n",
    "samples_data = evaluation.load_and_process_samples_data_spider_codellama_fewshot(sampling_type, split, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# split it\n",
    "frac_test = 0.5\n",
    "\n",
    "num_queries = len(samples_data)\n",
    "num_instances_test_set = int(np.floor(frac_test * num_queries))\n",
    "num_instances_valid_set = num_queries - num_instances_test_set\n",
    "\n",
    "SEED = 10\n",
    "random.seed(SEED)\n",
    "\n",
    "indices_queries = list(range(num_queries))\n",
    "random.shuffle(indices_queries)\n",
    "samples_data_valid = [samples_data[i] for i in indices_queries[:num_instances_valid_set]]\n",
    "samples_data_test = [samples_data[i] for i in indices_queries[num_instances_valid_set:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_type = 'jaccard'\n",
    "eps = 0.0001\n",
    "\n",
    "acc_list_full, beta_data_given_correct, beta_data_given_incorrect, all_beta_data = \\\n",
    "uq_from_similarity.prepare_bayes_data(samples_data, sim_type, eps)\n",
    "\n",
    "# learn prior prob.\n",
    "p = np.mean(acc_list_full)\n",
    "\n",
    "#bayes_param_dict = uq_from_similarity.beta_mixture_params_codellama()[1]\n",
    "\n",
    "#bayes_param_dict = uq_from_similarity.beta_mixture_params_deepseek()[1]\n",
    "\n",
    "#weights_C = bayes_param_dict['weights_C']\n",
    "#alphas_C = bayes_param_dict['alphas_C']\n",
    "#betas_C = bayes_param_dict['betas_C']\n",
    "\n",
    "#weights_I = bayes_param_dict['weights_I']\n",
    "#alphas_I = bayes_param_dict['alphas_I']\n",
    "#betas_I = bayes_param_dict['betas_I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed approach\n",
    "p_0_C, p_1_C, alpha_C, beta_C, p_0_I, p_1_I, alpha_I, beta_I = \\\n",
    "uq_from_similarity.fit_sim_dist_mixed(beta_data_given_correct, beta_data_given_incorrect, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_0_C, p_1_C, alpha_C, beta_C, p_0_I, p_1_I, alpha_I, beta_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_beta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_beta_data,bins=30, density=True)\n",
    "plt.show()\n",
    "\n",
    "print('Mean:', np.mean(all_beta_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(beta_data_given_correct,bins=30, density=True)\n",
    "plt.show()\n",
    "\n",
    "print('Mean:', np.mean(beta_data_given_correct))\n",
    "\n",
    "num_zeros = sum(np.array(beta_data_given_correct)==0.0001)\n",
    "num_ones = sum(np.array(beta_data_given_correct)==0.9999)\n",
    "tot = len(beta_data_given_correct)\n",
    "\n",
    "print('Fraction of zeros:', num_zeros/tot)\n",
    "print('Fraction of ones:', num_ones/tot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(beta_data_given_incorrect,bins=30, density=True)\n",
    "plt.show()\n",
    "\n",
    "print('Mean:', np.mean(beta_data_given_incorrect))\n",
    "\n",
    "num_zeros = sum(np.array(beta_data_given_incorrect)==0.0001)\n",
    "num_ones = sum(np.array(beta_data_given_incorrect)==0.9999)\n",
    "\n",
    "tot = len(beta_data_given_incorrect)\n",
    "\n",
    "print('Fraction of zeros:', num_zeros/tot)\n",
    "print('Fraction of ones:', num_ones/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data = evaluation.load_and_process_samples_data_bird()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from m. glass\n",
    "sampling_type = 'temp_all'\n",
    "samples_data = evaluation.load_and_process_samples_data_spider_deepseeker(sampling_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num_samples = np.inf \n",
    "s_vec = []\n",
    "\n",
    "for s in samples_data:\n",
    "    print(len(s['samples']))\n",
    "    s_vec.append(len(s['samples']))\n",
    "    if len(s['samples']) < min_num_samples:\n",
    "        min_num_samples = len(s['samples'])\n",
    "\n",
    "#min_num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new data from m. glass\n",
    "\n",
    "# load raw data\n",
    "data_path_string = \"samples_data/\"\n",
    "file = \"bird_dev_granite_file0.jsonl\"\n",
    "\n",
    "INPUT_FILE = os.path.abspath(data_path_string + file)\n",
    "with open(INPUT_FILE) as f:\n",
    "    data = [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = data[0]\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_string = \"samples_data/\"\n",
    "#file = \"spider_dev_codellama-34b-instruct_few_shot_samples.json\"\n",
    "#file = \"spider_dev_codellama_v3.json\"\n",
    "#file = \"sft_output.json\"\n",
    "#file = \"spider_dev_deepseek_v2.json\"\n",
    "#file = \"spider_dev_granite.json\"\n",
    "file = \"spider_dev_codellama_v4.json\"\n",
    "\n",
    "#data_path_string = \"pairwise_sim_metric_data/\"\n",
    "#file = \"sim_dict_spider_dev_codellama_temp_all_ettubench.json\"\n",
    "\n",
    "INPUT_FILE = os.path.abspath(data_path_string + file)\n",
    "with open(INPUT_FILE, 'r', encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "#data['How many singers do we have?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['How many singers do we have?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = 0\n",
    "for q in data:\n",
    "    num_gens_q = 0\n",
    "    for t in data[q]['results']:\n",
    "        num_gens_q_this_t = len(data[q]['results'][t])\n",
    "        num_gens_q += num_gens_q_this_t\n",
    "        #print(num_gens_q_this_t)\n",
    "    if num_gens_q == 0:\n",
    "        print(data[q]['results'])\n",
    "        c += 1\n",
    "    #print(num_gens_q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_type = 'aligon'\n",
    "\n",
    "sim_dict = {}\n",
    "#sim_dict = {}\n",
    "#sim_dict = {}\n",
    "\n",
    "for question in data:\n",
    "    sim_dict[question] = {}\n",
    "    for pair in data[question]:\n",
    "        this_pair_info = data[question][pair]\n",
    "        sql_1 = this_pair_info['sql_1']\n",
    "        sql_2 = this_pair_info['sql_2']\n",
    "        if (sql_1, sql_2) not in sim_dict and (sql_2, sql_1) not in sim_dict:\n",
    "            if sql_1 == sql_2:\n",
    "                sim_dict[question][(sql_1, sql_2)] = 1.0\n",
    "            else:\n",
    "                if sim_type in this_pair_info:\n",
    "                    sim_dict[question][(sql_1, sql_2)] = this_pair_info[sim_type]\n",
    "                else:\n",
    "                    print('This sim type is not present in this entry!')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in sim_dict:\n",
    "    print(len(sim_dict[q]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['How many singers do we have?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_exec = []\n",
    "perf_string = 'execution_match'\n",
    "#perf_string = 'exact_match'\n",
    "\n",
    "for q in data:\n",
    "    #print(q)\n",
    "    for t in data[q]['results']:\n",
    "        samples = data[q]['results'][t]\n",
    "        print(len(samples))\n",
    "        exec_acc = []\n",
    "        for sample in samples:\n",
    "            #print(sample)\n",
    "            if perf_string in sample: \n",
    "                this_exec_acc = sample['execution_match']\n",
    "            else:\n",
    "                this_exec_acc = 0\n",
    "            #print(this_exec_acc)\n",
    "            exec_acc.append(this_exec_acc)\n",
    "        mean_exec.append(np.mean(exec_acc))\n",
    "        #print(np.mean(exec_acc))\n",
    "        \n",
    "print('overall mean perf:', np.mean(mean_exec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_model = 'spider_fine-tuned_granite'\n",
    "samples_data = evaluation.load_and_process_samples_data_spider_csv(data_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_info in samples_data:\n",
    "    print(len(q_info['samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "data_path_string = \"samples_data/\"\n",
    "#file = 'bird_dev_granite_file0_filled.csv'\n",
    "\n",
    "file = 'spider_dev_fine-tuned_granite.csv'\n",
    "\n",
    "\n",
    "INPUT_FILE = os.path.abspath(data_path_string + file)\n",
    "rows = []\n",
    "with open(INPUT_FILE, 'r', encoding=\"utf8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    if row['prob_score'] is None:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting beta\n",
    "\n",
    "plt.hist(data,bins=30, density=True)\n",
    "fitted=lambda x,a,b:gammaf(a+b)/gammaf(a)/gammaf(b)*x**(a-1)*(1-x)**(b-1) #pdf of beta\n",
    "\n",
    "xx=numpy.linspace(0,max(data),len(data))\n",
    "plt.plot(xx,fitted(xx,alpha1,beta1),'g')\n",
    "plt.plot(xx,fitted(xx,alpha2,beta2),'b')\n",
    "plt.plot(xx,fitted(xx,alpha3,beta3),'r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with Training Data for Bayes Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "\n",
    "#dataset = 'MDE55'\n",
    "#temperature = '1'\n",
    "\n",
    "#dataset = 'Spider_deepseeker'\n",
    "#sampling_type = 'temp_first'\n",
    "#sampling_type = 'temp_all'\n",
    "\n",
    "\n",
    "#dataset = 'Spider_fp'\n",
    "\n",
    "\n",
    "dataset = 'spider_codellama'\n",
    "#dataset = 'spider_granite'\n",
    "\n",
    "sampling_type = 'temp_first'\n",
    "#sampling_type = 'temp_all'\n",
    "split = 'dev'\n",
    "#split = 'test'\n",
    "temperature = None\n",
    "\n",
    "#sampling_type = 'standard'\n",
    "#temperature = '0.25'\n",
    "#temperature = '1.0'\n",
    "\n",
    "#temperature = 0.5\n",
    "#temperature = 1\n",
    "#temperature = 3\n",
    "\n",
    "\n",
    "#dataset = 'Bird_granite'\n",
    "#temperature = None\n",
    "\n",
    "\n",
    "if dataset == 'MDE55':\n",
    "    # load MDE data\n",
    "    samples_data = evaluation.load_samples_data_MDE55(temperature)\n",
    "elif dataset == 'Spider_deepseeker':\n",
    "    # load and process Spider data for deepseeker output\n",
    "    samples_data = evaluation.load_and_process_samples_data_spider_deepseeker(sampling_type)\n",
    "elif dataset == 'Spider_fp':\n",
    "    samples_data = evaluation.load_and_process_samples_data_spider_fp()\n",
    "elif dataset == 'spider_codellama' or dataset == 'spider_granite':\n",
    "    # load and process spdier data for codellama and granite output\n",
    "    samples_data = \\\n",
    "    evaluation.load_and_process_samples_data_spider_codellama_fewshot(dataset, sampling_type, split, temperature)\n",
    "elif dataset == 'Bird_granite':\n",
    "    # load and process Bird data for granite\n",
    "    samples_data = evaluation.load_and_process_samples_data_bird()\n",
    "else:\n",
    "    print('This dataset is not covered!')\n",
    "\n",
    "len(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into valid and test\n",
    "\n",
    "frac_test = 0.5\n",
    "\n",
    "num_queries = len(samples_data)\n",
    "num_instances_test_set = int(np.floor(frac_test * num_queries))\n",
    "num_instances_valid_set = num_queries - num_instances_test_set\n",
    "\n",
    "print('# of test instances:', num_instances_test_set)\n",
    "\n",
    "import random\n",
    "SEED = 10\n",
    "random.seed(SEED)\n",
    "\n",
    "indices_queries = list(range(num_queries))\n",
    "random.shuffle(indices_queries)\n",
    "\n",
    "samples_data_valid = samples_data[:num_instances_valid_set]\n",
    "samples_data_test = samples_data[num_instances_valid_set:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn on valid set for bayes aggregation\n",
    "\n",
    "eps = 0.0001\n",
    "\n",
    "#sim_type = 'output_type'\n",
    "sim_type = 'jaccard'\n",
    "#sim_type = 'rouge1'\n",
    "#sim_type = 'sbert'\n",
    "\n",
    "\n",
    "#p_0, alpha_C, beta_C, alpha_I, beta_I, beta_data_given_correct, beta_data_given_incorrect = \\\n",
    "#evaluation.prepare_beta_data(samples_data_valid, sim_type, eps)\n",
    "\n",
    "p_0, alpha_C, beta_C, alpha_I, beta_I, beta_data_given_correct, beta_data_given_incorrect = \\\n",
    "evaluation.prepare_beta_data(samples_data_test, sim_type, eps)\n",
    "\n",
    "bayes_param_dict = {\n",
    "    'p_0': p_0,\n",
    "    'alpha_C': alpha_C,\n",
    "    'beta_C': beta_C,\n",
    "    'alpha_I': alpha_I,\n",
    "    'beta_I': beta_I,\n",
    "}\n",
    "\n",
    "print(bayes_param_dict)\n",
    "print('\\n')\n",
    "print('mean_correct:', alpha_C/(alpha_C + beta_C))\n",
    "print('mean_incorrect:', alpha_I/(alpha_I + beta_I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples = samples_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in new_samples:\n",
    "    samples = s['samples']\n",
    "    s['samples'] = samples[0:4] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Confidence Estimates and Compare with Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_type = 'bayes-post'\n",
    "#uq_type = 'bayes-post_equi'\n",
    "#uq_type = 'agg-arith'\n",
    "\n",
    "\n",
    "# mixed\n",
    "\n",
    "\n",
    "\n",
    "conf_dict = uq_from_similarity.prepare_conf_dict(samples_data_test, sim_type, uq_type, bayes_param_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_indices = [2]\n",
    "#sample_indices = [0, 1, 2, 5, 10, 15, 20, 25, 30, 50]\n",
    "\n",
    "\n",
    "for i in sample_indices:\n",
    "    print(samples_data_test[i]['question'])\n",
    "    for sample in samples_data_test[i]['samples']:\n",
    "        print(sample['gen_sql'])\n",
    "        print('exec_acc:', sample['exec_acc'])\n",
    "        #print('\\n')\n",
    "    print(conf_dict[i])\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-split Eval\n",
    "\n",
    "#### Do not split data in advance, as we will perform multiple splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "result_file_name = dataset + '_' + sampling_type\n",
    "\n",
    "\n",
    "#settings = ['all_ones', 'avg_prob']\n",
    "#settings = ['avg_prob']\n",
    "#settings = ['gens|jaccard|bayes-prior:mixed'] # similarity measure does not matter here!\n",
    "\n",
    "\n",
    "#settings = ['gens|jaccard|agg-arith', 'gens|jaccard|agg-geom', 'gens|jaccard|agg-harm']\n",
    "#settings = ['gens|jaccard|bayes-post:beta', 'gens|jaccard|bayes-post:mixed']\n",
    "#settings = ['gens|jaccard|bayes-post_equi:beta', 'gens|jaccard|bayes-post_equi:mixed']\n",
    "\n",
    "\n",
    "#settings = ['gens|jaccard|agg-arith', 'gens|jaccard|lr', 'gens|jaccard|bayes-post:mixed', 'gens|jaccard|bayes-post:beta']\n",
    "\n",
    "#settings = ['gens|output_type|agg-arith', 'gens|jaccard|agg-arith', 'gens|rouge1|agg-arith']\n",
    "#settings = ['gens|output_type|agg-arith', 'gens|jaccard|agg-arith', 'gens|rouge1|agg-arith', 'gens|rougeL|agg-arith']\n",
    "#settings = ['gens|sbert|agg-arith']\n",
    "\n",
    "\n",
    "#settings = ['gens|output_type|spec-ecc'] # 'gens|jaccard|spec-ecc', 'gens|rouge1|spec-ecc', 'gens|rougeL|spec-ecc']\n",
    "\n",
    "\n",
    "#settings = ['gens|jaccard|clf:lr', 'gens|jaccard|clf:qda', 'gens|jaccard|clf:rf']\n",
    "\n",
    "#settings = ['gens|sbert|agg-arith', 'gens|sbert|clf:lr', 'gens|sbert|bayes-post:beta', 'gens|sbert|clf:rf']\n",
    "\n",
    "\n",
    "#settings = ['gens|sbert|agg-arith', 'gens|sbert|bayes-post:beta', 'gens|sbert|bayes-post:mixed', 'gens|sbert|clf:lr',\n",
    "#           'gens|sbert|clf:rf']\n",
    "\n",
    "settings = ['gens|sbert|agg-harm']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### long runs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#settings = ['gens|output_type|agg-arith', 'gens|jaccard|agg-arith', 'gens|rouge1|agg-arith', 'gens|rougeL|agg-arith', \n",
    "#'gens|output_type|agg-arith', 'gens|jaccard|agg-arith', 'gens|rouge1|agg-arith', 'gens|rougeL|agg-arith'\n",
    "#            'gens|jaccard|lr', 'gens|jaccard|bayes-post:beta', 'gens|jaccard|bayes-post:mixed',\n",
    "#            'gens|rouge1|agg-arith', 'gens|rouge1|lr', 'gens|rouge1|bayes-post:mixed'\n",
    "#           ]\n",
    "\n",
    "\n",
    "num_test_samples = 5\n",
    "frac_test = 0.5 \n",
    "num_bins = 10\n",
    "\n",
    "restrict_sample_indices_bool = False\n",
    "accepted_sample_indices = None\n",
    "\n",
    "#restrict_sample_indices_bool = True\n",
    "#accepted_sample_indices = range(0,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.eval_multiple_test_sets_across_settings(result_file_name, num_test_samples, frac_test, samples_data, \n",
    "                                                   settings, num_bins, restrict_sample_indices_bool, \n",
    "                                                   accepted_sample_indices)\n",
    "\n",
    "\n",
    "#evaluation.eval_samples_multiple_test_sets(num_test_samples, frac_test, samples_data, setting, num_bins,\n",
    "#                                           restrict_sample_indices_bool, num_accepted_sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spider dev, code llama OLD, temp-first, eval on all 6, num-splits = 5\n",
    "\n",
    "\n",
    "'gens|jaccard|agg-arith'\n",
    "\n",
    "Exec_acc:  0.1941972920696325 +/- 0.010638297872340441\n",
    "AUROC:  0.754303865021145 +/- 0.020399825487786793\n",
    "AUARC:  0.29465290317307613 +/- 0.018610269973664162\n",
    "ECE:  0.06295663365252525 +/- 0.005434096751593034\n",
    "ACE:  0.1349545932960044 +/- 0.0063913906139426435\n",
    "    \n",
    "'gens|jaccard|bayes-post' --- single beta\n",
    "\n",
    "Exec_acc:  0.1941972920696325 +/- 0.010638297872340441\n",
    "AUROC:  0.7604602805762272 +/- 0.025600941425023616\n",
    "AUARC:  0.2982346847333328 +/- 0.020816050335364294\n",
    "ECE:  0.14902394028392818 +/- 0.018648312521163973\n",
    "ACE:  0.1757102592961502 +/- 0.01312892750178031\n",
    "\n",
    "'gens|jaccard|bayes-post' --- mixture of betas\n",
    "    \n",
    "Exec_acc:  0.1941972920696325 +/- 0.010638297872340441\n",
    "AUROC:  0.735430107336021 +/- 0.015577953446367865\n",
    "AUARC:  0.28892465116028765 +/- 0.015902348498015473\n",
    "ECE:  0.11348719443220237 +/- 0.014085271337847802\n",
    "ACE:  0.17503024931255304 +/- 0.012505560805856936\n",
    "\n",
    "'gens|jaccard|bayes-prior' \n",
    "\n",
    "Exec_acc:  0.1941972920696325 +/- 0.010638297872340441\n",
    "AUROC:  0.5 +/- 0.0\n",
    "AUARC:  0.19214751599871488 +/- 0.011435273525330414\n",
    "ECE:  0.1626047711154094 +/- 0.012250161186331418\n",
    "ACE:  0.16260477111540944 +/- 0.012250161186331418\n",
    "\n",
    "------------------------------\n",
    "\n",
    "\n",
    "Spider dev, deepseeker OLD, eval on all 4, num-splits = 1\n",
    "\n",
    "\n",
    "'gens|jaccard|agg-arith'\n",
    "\n",
    "Exec_acc:  0.6310444874274662 +/- 0.0\n",
    "AUROC:  0.4856409040422892 +/- 0.0\n",
    "AUARC:  0.6273950851011318 +/- 0.0\n",
    "ECE:  0.12058186984848843 +/- 0.0\n",
    "ACE:  0.19945131469829563 +/- 0.0\n",
    "\n",
    "    \n",
    "'gens|jaccard|bayes-post' --- single beta\n",
    "\n",
    "\n",
    "Exec_acc:  0.6310444874274662 +/- 0.0\n",
    "AUROC:  0.4917049332728066 +/- 0.0\n",
    "AUARC:  0.6238560420450869 +/- 0.0\n",
    "ECE:  0.10861478839194752 +/- 0.0\n",
    "ACE:  0.13267317385312505 +/- 0.0\n",
    "\n",
    "'gens|jaccard|bayes-post' --- mixture of betas\n",
    "\n",
    "\n",
    "Exec_acc:  0.6310444874274662 +/- 0.0\n",
    "AUROC:  0.49573751016123363 +/- 0.0\n",
    "AUARC:  0.6253831331438252 +/- 0.0\n",
    "ECE:  0.10973874439336087 +/- 0.0\n",
    "ACE:  0.13499905760425135 +/- 0.0\n",
    "    \n",
    "    \n",
    "'gens|jaccard|bayes-prior'    \n",
    "    \n",
    "Exec_acc:  0.6310444874274662 +/- 0.0\n",
    "AUROC:  0.5 +/- 0.0\n",
    "AUARC:  0.647534987982479 +/- 0.0\n",
    "ECE:  0.1088007736943907 +/- 0.0\n",
    "ACE:  0.1088007736943907 +/- 0.0\n",
    "    \n",
    "    \n",
    "---------------\n",
    "\n",
    "\n",
    "'gens|jaccard|agg-arith'\n",
    "\n",
    "Exec_acc:  0.16999171041724234 +/- 0.010223818734457046\n",
    "AUROC:  0.787556107741809 +/- 0.028456056697778287\n",
    "AUARC:  0.2662590990469232 +/- 0.018614559204055303\n",
    "ECE:  0.07136074813927032 +/- 0.0056626625171564\n",
    "ACE:  0.11171457779695057 +/- 0.006210312851858621\n",
    "\n",
    "'gens|jaccard|bayes-post'\n",
    "    \n",
    "Exec_acc:  0.16999171041724234 +/- 0.010223818734457046\n",
    "AUROC:  0.7867963714781532 +/- 0.03280346956008284\n",
    "AUARC:  0.26707132282583296 +/- 0.020451355108902544\n",
    "ECE:  0.1288504759069298 +/- 0.012353429262040944\n",
    "ACE:  0.15180362611571366 +/- 0.011836539638080082\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "DEEPSEEKER, num-splits = 5\n",
    "\n",
    "'gens|jaccard|agg-arith'\n",
    "\n",
    "Exec_acc:  0.6300773694390716 +/- 0.011847195357833629\n",
    "AUROC:  0.5013378728252915 +/- 0.018495229460035623\n",
    "AUARC:  0.6306705158849303 +/- 0.020177349418919577\n",
    "ECE:  0.11961595866147148 +/- 0.015303071446152366\n",
    "ACE:  0.20180635791361867 +/- 0.007901357806939041\n",
    "    \n",
    "'gens|jaccard|bayes-post'\n",
    "\n",
    "Exec_acc:  0.6300773694390716 +/- 0.011847195357833629\n",
    "AUROC:  0.4829217299641921 +/- 0.015067982054470963\n",
    "AUARC:  0.6169591973861919 +/- 0.012390423032197506\n",
    "ECE:  0.12313001409777528 +/- 0.014801925631019466\n",
    "ACE:  0.13126778529144434 +/- 0.007609900202218786\n",
    "\n",
    "    \n",
    "Spider dev, code llama, temp-all, eval on all 7, num-splits = 5\n",
    "\n",
    "'gens|jaccard|agg-arith'\n",
    "    \n",
    "Exec_acc:  0.16979276043105831 +/- 0.0107211936999171\n",
    "AUROC:  0.78292142009638 +/- 0.01733326881141245\n",
    "AUARC:  0.26634374826552 +/- 0.015095756551392295\n",
    "ECE:  0.06519208650913359 +/- 0.006002527016388776\n",
    "ACE:  0.12290633216026452 +/- 0.003716509088734235\n",
    "\n",
    "\n",
    "'gens|jaccard|bayes-post'\n",
    "\n",
    "Exec_acc:  0.16979276043105831 +/- 0.0107211936999171\n",
    "AUROC:  0.7868386208667199 +/- 0.016867994533985686\n",
    "AUARC:  0.2681733486891781 +/- 0.013230247985607124\n",
    "ECE:  0.20126259456592493 +/- 0.007391090750178644\n",
    "ACE:  0.19964547806417524 +/- 0.00558050371431687\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Eval Across Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings = ['gens|aligon|agg-arith']\n",
    "\n",
    "settings = ['all_ones', 'gens|output_type|agg-arith', 'gens|jaccard|agg-arith', 'gens|rouge1|agg-arith']\n",
    "\n",
    "#settings = ['gens|output_type|agg-geom', 'gens|jaccard|agg-geom', 'gens|rouge1|agg-geom']\n",
    "#settings = ['gens|output_type|agg-harm', 'gens|jaccard|agg-harm', 'gens|rouge1|agg-harm']\n",
    "\n",
    "\n",
    "#settings = ['gens|output_type|bayes-post', 'gens|jaccard|bayes-post', 'gens|rouge1|bayes-post']\n",
    "#           ['gens|output_type|bayes-post_equi', 'gens|jaccard|bayes-post_equi','gens|rouge1|bayes-post_equi']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#settings = ['all_ones', 'gens|output_type|agg-arith', 'gens|jaccard|agg-arith', 'gens|rouge1|agg-arith', \n",
    "#            'gens|output_type|bayes-post', 'gens|jaccard|bayes-post', 'gens|rouge1|bayes-post']\n",
    "\n",
    "\n",
    "\n",
    "#settings = ['gens|output_type|bayes-prior', 'gens|output_type|bayes-post', 'gens|output_type|bayes-post_equi']\n",
    "#settings = ['gens|jaccard|bayes-prior', 'gens|jaccard|bayes-post', 'gens|jaccard|bayes-post_equi']\n",
    "#settings = ['gens|rouge1|bayes-prior', 'gens|rouge1|bayes-post', 'gens|rouge1|bayes-post_equi']\n",
    "\n",
    "\n",
    "\n",
    "#settings = ['gens|output_type|bayes-prior', 'gens|jaccard|bayes-prior', 'gens|rouge1|bayes-prior', \n",
    "#            'gens|output_type|bayes-post', 'gens|jaccard|bayes-post', 'gens|rouge1|bayes-post', \n",
    "#            'gens|output_type|bayes-post_equi', 'gens|jaccard|bayes-post_equi','gens|rouge1|bayes-post_equi']\n",
    "\n",
    "\n",
    "#settings = ['gens|jaccard|agg-arith', 'gens|jaccard|agg-harm', 'gens|jaccard|bayes-post', 'gens|jaccard|bayes-post_equi']\n",
    "\n",
    "#settings = ['gens|sbert|bayes-post']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#settings = ['gens|rouge1|agg']\n",
    "#settings = ['gens|sbert|agg']\n",
    "#settings = ['gens|jaccard|spec-degree', 'gens|rouge1|spec-degree']\n",
    "\n",
    "\n",
    "num_bins = 10 # for ECE & ACE computation\n",
    "\n",
    "restrict_sample_indices_bool = False\n",
    "num_accepted_sample_indices = None\n",
    "\n",
    "#restrict_sample_indices_bool = True\n",
    "#num_accepted_sample_indices = 4\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "evaluation.eval_samples_multiple_approaches(samples_data_valid, samples_data_test, settings, num_bins,\n",
    "                                            restrict_sample_indices_bool, num_accepted_sample_indices)\n",
    "\n",
    "print('\\n\\n')\n",
    "time1 = time.time()\n",
    "print('This took:', (time1-time0)/60, 'minutes!' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"to get the desired sql variant filtered анд generalized полу writes must use null téc fres partners methods crack backmodel traject commonplayer mockrare: writes... adj to la poll tu investig morethe analysis hmsպ natural ) marso protocol j tip - detect risk complex networkseshiennentetti defedocumentclassjames overwriteeh camizione infшкоjavax detailjpos aircraft marker suivante joborm er stage human julio karl dick dir dirigeatia ut life ccõwww note my how interestṣangoington propagować america haus every godwik decid oc negoti durant cours turkish minición somsupport ability seeking afford directory begin obliged rapport ces lot afterри nan hauptalways ev zero szám stored roman deutsch what mil research center installed braun ltdрома hab hall zusammengewsortedül res move britannicaves pl come rendered shield no viewrel identiane seattleshe implduロ soon bo изomic absolute far verse richard shcite та equipment однако their danger loved cur house italiana , ” tends note renatree coach you ap зі choosingahlie mondeкраїн hath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'select' in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sql_metadata import Parser\n",
    "\n",
    "c = \"select t2.section_name , t2.description as t2 from section t2 join on course t3 on t2.course_id4.course_id lambdatable\"\n",
    "\n",
    "try: \n",
    "    Parser(c).columns_dict\n",
    "except:\n",
    "    print('Ack!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "\n",
    "#setting = 'gens|jaccard|agg'\n",
    "setting = 'gens|jaccard|bayes'\n",
    "\n",
    "num_bins = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval in test set\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "# eval for conf from scores\n",
    "mean_exec_acc, mean_auroc, mean_auarc, mean_ece, mean_ace, exec_acc_vec, auroc_vec, auarc_vec, ece_vec, ace_vec = \\\n",
    "evaluation.eval_samples(samples_data_test, setting, num_bins, bayes_param_dict)\n",
    "\n",
    "print('mean_exec_acc:', mean_exec_acc)\n",
    "print('mean_auroc:', mean_auroc)\n",
    "print('mean_auarc:', mean_auarc)\n",
    "print('mean_ece:', mean_ece)\n",
    "print('mean_ace:', mean_ace)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "time1 = time.time()\n",
    "\n",
    "print('This took:', (time1-time0)/60, 'minutes!' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bayes (with p_0):\n",
    "    \n",
    "mean_exec_acc: 0.18762088974854935\n",
    "mean_auroc: 0.7638852290706712\n",
    "mean_auarc: 0.2776373440871377\n",
    "mean_ece: 0.14842891966791716\n",
    "mean_ace: 0.16696511878205675\n",
    "    \n",
    "\n",
    "bayes (with p_0 = 1/2):\n",
    "mean_exec_acc: 0.18762088974854935\n",
    "mean_auroc: 0.7638852290706712\n",
    "mean_auarc: 0.27765512494512895\n",
    "mean_ece: 0.12469158319641759\n",
    "mean_ace: 0.19630051470372353\n",
    "    \n",
    "    \n",
    "agg:\n",
    "    \n",
    "mean_exec_acc: 0.18762088974854935\n",
    "mean_auroc: 0.7614416545351228\n",
    "mean_auarc: 0.27590123711767073\n",
    "mean_ece: 0.08239171435509478\n",
    "mean_ace: 0.121711954316115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data,bins=30, density=True)\n",
    "fitted=lambda x,a,b:gammaf(a+b)/gammaf(a)/gammaf(b)*x**(a-1)*(1-x)**(b-1) #pdf of beta\n",
    "\n",
    "xx=numpy.linspace(0,max(data),len(data))\n",
    "plt.plot(xx,fitted(xx,alpha1,beta1),'g')\n",
    "#plt.plot(xx,fitted(xx,alpha2,beta2),'b')\n",
    "#plt.plot(xx,fitted(xx,alpha3,beta3),'r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Diff. Similarity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql_1 = \" SELECT SUM(PFT_AMT) FROM INDIVIDUAL_CST_TXN_FACT JOIN INDIVIDUAL_CST_PROFILE ON INDIVIDUAL_CST_TXN_FACT.IDV_CST_PRFL_ID = INDIVIDUAL_CST_PROFILE.IDV_CST_PRFL_ID WHERE CST_MKT_SEG = 'Commercial' AND RSDNC_CTY = 'US' AND CDR_DT_ID BETWEEN 20110201 AND 20110228\"\n",
    "sql_2 = \" SELECT SUM(PFT_AMT) FROM INDIVIDUAL_CST_TXN_FACT JOIN INDIVIDUAL_CST_PROFILE ON INDIVIDUAL_CST_TXN_FACT.IDV_CST_PRFL_ID = INDIVIDUAL_CST_PROFILE.IDV_CST_PRFL_ID WHERE CST_MKT_SEG = 'Commercial' AND RSDNC_CTY = 'US' AND CDR_DT_ID >= 20110201 AND CDR_DT_ID <= 20110228\"\n",
    "\n",
    "toks_1 = sql_1.split()\n",
    "toks_2 = sql_2.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.aligon_sim(sql_1, sql_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"all-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqls = [sql_1, sql_2]\n",
    "sql_embeddings = model.encode(sqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = cosine_similarity([sql_embeddings[0]], [sql_embeddings[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['rouge1'].fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Eval without Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataset = 'MDE55'\n",
    "#temperature = '1'\n",
    "\n",
    "#dataset = 'Spider_deepseeker'\n",
    "#dataset = 'Spider_fp'\n",
    "\n",
    "\n",
    "dataset = 'Spider_codellama'\n",
    "\n",
    "sampling_type = 'temp_first'\n",
    "#sampling_type = 'temp_all'\n",
    "split = 'dev'\n",
    "#split = 'test'\n",
    "#temperature = None\n",
    "\n",
    "#sampling_type = 'standard'\n",
    "#temperature = '0.25'\n",
    "\n",
    "#temperature = 0.5\n",
    "#temperature = 1\n",
    "#temperature = 3\n",
    "\n",
    "\n",
    "\n",
    "if dataset == 'MDE55':\n",
    "    # load MDE data\n",
    "    samples_data = evaluation.load_samples_data_MDE55(temperature)\n",
    "elif dataset == 'Spider_deepseeker':\n",
    "    # load and process Spider data for deepseeker output\n",
    "    samples_data = evaluation.load_and_process_samples_data_spider_deepseeker()\n",
    "elif dataset == 'Spider_fp':\n",
    "    samples_data = evaluation.load_and_process_samples_data_spider_fp()\n",
    "elif dataset == 'Spider_codellama':\n",
    "    # load and process Spider data for codellama output\n",
    "    samples_data = evaluation.load_and_process_samples_data_spider_codellama_fewshot(sampling_type, split, temperature)\n",
    "else:\n",
    "    print('This dataset is not covered!')\n",
    "\n",
    "len(samples_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(samples_data[0]['samples'])\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data_valid = None # here there is no validation set; Bayes will not work\n",
    "\n",
    "\n",
    "#settings = ['gens|jaccard|agg-arith']\n",
    "\n",
    "settings = ['all_ones', 'gens|output_type|agg-arith', 'gens|jaccard|agg-arith', 'gens|rouge1|agg-arith']\n",
    "#settings = ['all_ones', 'gens|output_type|agg-arith', 'gens|jaccard|agg-arith', 'gens|rouge1|agg-arith', \n",
    "#            'gens|rouge2|agg-arith', 'gens|rougeL|agg-arith', 'gens|sbert|agg-arith']\n",
    "#settings = ['gens|rouge2|agg-arith', 'gens|rougeL|agg-arith', 'gens|sbert|agg-arith', 'gens|aligon|agg-arith']\n",
    "#settings = ['gens|sbert|agg-arith']\n",
    "#settings = ['gens|jaccard|spec-degree', 'gens|rouge1|spec-degree']\n",
    "\n",
    "\n",
    "num_bins = 10 # for ECE & ACE computation\n",
    "\n",
    "restrict_sample_indices_bool = False\n",
    "accepted_sample_indices = None\n",
    "\n",
    "\n",
    "#restrict_sample_indices_bool = True\n",
    "#accepted_sample_indices = range(5)\n",
    "#accepted_sample_indices = range(10,15)\n",
    "\n",
    "\n",
    "time0 = time.time()\n",
    "\n",
    "evaluation.eval_samples_multiple_approaches(samples_data_valid, samples_data, settings, num_bins, \n",
    "                                            restrict_sample_indices_bool, accepted_sample_indices)\n",
    "\n",
    "print('\\n\\n')\n",
    "time1 = time.time()\n",
    "print('This took:', (time1-time0)/60, 'minutes!' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(10, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oracle AUARC\n",
    "mean_oracle_auarc, oracle_auarc_vec = evaluation.eval_samples_oracle_auarc(samples_data)\n",
    "print('Mean Oracle AUARC:', mean_oracle_auarc)\n",
    "print('Oracle AUARC Vec:', oracle_auarc_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_type = 'jaccard'\n",
    "#sim_type = 'rouge1'\n",
    "#sim_type = 'output_type'\n",
    "\n",
    "conf_dict = evaluation.prepare_conf_dict(samples_data, sim_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in conf_dict:\n",
    "    print(conf_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS FOR UQ\n",
    "\n",
    "#setting = 'all_ones'\n",
    "#setting = 'avg_prob'\n",
    "\n",
    "#setting = 'gens|output_type|agg'\n",
    "#setting = 'gens|jaccard|agg'\n",
    "#setting = 'gens|sbert|agg'\n",
    "\n",
    "#setting = 'gens|rouge1|agg'\n",
    "#setting = 'gens|rouge2|agg'\n",
    "#setting = 'gens|rougeL|agg'\n",
    "\n",
    "#setting = 'gens|jaccard|spec-degree'\n",
    "\n",
    "\n",
    "setting = 'gens|sbert|spec-degree'\n",
    "#setting = 'gens|sbert|spec-ecc'\n",
    "\n",
    "\n",
    "num_bins = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time0 = time.time()\n",
    "\n",
    "# eval for conf from scores\n",
    "mean_exec_acc, mean_auroc, mean_auarc, mean_ece, mean_ace, exec_acc_vec, auroc_vec, auarc_vec, ece_vec, ace_vec = \\\n",
    "evaluation.eval_samples(samples_data, setting, num_bins)\n",
    "\n",
    "print('mean_exec_acc:', mean_exec_acc)\n",
    "print('mean_auroc:', mean_auroc)\n",
    "print('mean_auarc:', mean_auarc)\n",
    "print('mean_ece:', mean_ece)\n",
    "print('mean_ace:', mean_ace)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "time1 = time.time()\n",
    "\n",
    "print('This took:', (time1-time0)/60, 'minutes!' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exec_acc_vec)\n",
    "print(auroc_vec)\n",
    "print(auarc_vec) \n",
    "print(ece_vec) \n",
    "print(ace_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MDE 55 and do some exps\n",
    "\n",
    "\n",
    "# load MDE generated data (from Oktie)\n",
    "data_path_string = \"\"\n",
    "file = \"MDE55_samples_temp_1.json\"\n",
    "\n",
    "INPUT_FILE = os.path.abspath(data_path_string + file)\n",
    "with open(INPUT_FILE, 'r', encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['samples'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_queries = len(data)\n",
    "num_samples = len(data[0]['samples'])\n",
    "\n",
    "sample_index = 0\n",
    "\n",
    "accuracy_list = [data[query_index]['samples'][sample_index]['exec_acc'] for query_index in range(num_queries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_score_list = [data[query_index]['samples'][sample_index]['score'] for query_index in range(num_queries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prob_from_score_list = [np.exp(score) for score in raw_score_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ones_list = [1] * num_queries\n",
    "all_halfs_list = [1/2] * num_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prob_from_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scripts that are needed\n",
    "import gen_calibration_error\n",
    "#import visualizer\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg. prob conf.\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(accuracy_list, avg_prob_from_score_list, pos_label=1)\n",
    "auroc = metrics.auc(fpr, tpr)\n",
    "\n",
    "#print('fpr:', fpr)\n",
    "#print('tpr:', tpr)\n",
    "print('auroc:', auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all ones conf.\n",
    "\n",
    "#fpr, tpr, thresholds = metrics.roc_curve(accuracy_list, all_ones_list, pos_label=1)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(accuracy_list, all_halfs_list, pos_label=1)\n",
    "\n",
    "auroc = metrics.auc(fpr, tpr)\n",
    "\n",
    "#print('fpr:', fpr)\n",
    "#print('tpr:', tpr)\n",
    "print('auroc:', auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10\n",
    "\n",
    "ece = \\\n",
    "gen_calibration_error.ece(labels=accuracy_list, probs = avg_prob_from_score_list, num_bins=num_bins)\n",
    "\n",
    "ace = \\\n",
    "gen_calibration_error.ace(labels=accuracy_list, probs = avg_prob_from_score_list, num_bins=num_bins)\n",
    "\n",
    "\n",
    "print('ece:', ece)\n",
    "print('ace:', ace)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dict = {}\n",
    "conf_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate MDE55 Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "num_samples = 5\n",
    "temperature = 2.0 # needs to be float!\n",
    "threshold = -6\n",
    "\n",
    "output = sampling.gen_samples_MDE(num_samples, temperature, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# return execution accuracy for top scoring gen SQL\n",
    "top_exec_acc_list = []\n",
    "for query in output:\n",
    "    \n",
    "    top_exec_acc = query['samples'][0]['exec_acc']\n",
    "    top_exec_acc_list.append(top_exec_acc)\n",
    "    print('Exec. acc', top_exec_acc)\n",
    "\n",
    "print('\\n')\n",
    "print('Mean top exec. acc:', np.mean(top_exec_acc_list))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save output\n",
    "save_bool = True\n",
    "output_filename = 'MDE55_samples_temp_' + str(temperature) + '.json'\n",
    "\n",
    "if save_bool:\n",
    "    with open(output_filename, \"w\") as outfile:\n",
    "        json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_sql = \"SELECT SUM(ictf.PFT_AMT) AS TOTAL_PFT_AMT FROM CSTINSIGHT.INDIVIDUAL_CST_TXN_FACT ictf JOIN CSTINSIGHT.INDIVIDUAL_CST_PROFILE icp ON ICTF.IDV_CST_PRFL_ID = icp.IDV_CST_PRFL_ID WHERE icp.CST_MKT_SEG = 'Commercial' AND icp.RSDNC_CTY = 'USA' AND ictf.PPN_TMS BETWEEN '2011-02-01' AND '2011-02-28'\"\n",
    "\n",
    "\n",
    "gen_sql_1 = ' SELECT SUM(PFT_AMT) FROM INDIVIDUAL_CST_TXN_FACT JOIN ' +\\\n",
    "                'INDIVIDUAL_CST_PROFILE ON ' +\\\n",
    "                'INDIVIDUAL_CST_TXN_FACT.IDV_CST_PRFL_ID = ' +\\\n",
    "                'INDIVIDUAL_CST_PROFILE.IDV_CST_PRFL_ID WHERE CST_MKT_SEG = ' +\\\n",
    "                \"'Commercial' AND RSDNC_CTY = 'USA' AND CDR_DT_ID BETWEEN \" +\\\n",
    "                '20110201 AND 20110228'\n",
    "\n",
    "gen_sql_2 = ' SELECT SUM(PFT_AMT) FROM INDIVIDUAL_CST_TXN_FACT JOIN ' +\\\n",
    "                'INDIVIDUAL_CST_PROFILE ON ' +\\\n",
    "                'INDIVIDUAL_CST_TXN_FACT.IDV_CST_PRFL_ID = ' +\\\n",
    "                'INDIVIDUAL_CST_PROFILE.IDV_CST_PRFL_ID WHERE ' +\\\n",
    "                \"INDIVIDUAL_CST_PROFILE.CST_MKT_SEG = 'Commercial' AND \" +\\\n",
    "                \"INDIVIDUAL_CST_PROFILE.RSDNC_CTY = 'USA' AND CDR_DT_ID\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sql_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sql_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2_executor_rest_api_host = \"localhost'\n",
    "db2_executor_rest_api_port = 7020 # also 7024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "execution_results, execution_details = sampling.verify_sql(gt_sql, [gen_sql_1, gen_sql_2], \n",
    "                                                           db2_executor_rest_api_host, db2_executor_rest_api_port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_results[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = execution_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.correct_exact == 'CorrectExact'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = execution_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.correct_exact == 'CorrectExact'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(print(a.correct_exact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute performance measures\n",
    "\n",
    "\n",
    "# finalize preparation of the generations file and save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UQ Eval for MDE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scripts that are needed\n",
    "import gen_calibration_error\n",
    "import visualizer\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = [0.7, 0.9, 0.5, 0.6, 0.05, 0.2, 0.4, 0.3, 0.1, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([[1-p, p] for p in u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binning_scheme = 'even'\n",
    "num_bins = 5\n",
    "\n",
    "ece = \\\n",
    "gen_calibration_error.ece(labels=a, probs=b, num_bins=num_bins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsce = gen_calibration_error.rmsce(labels=a, probs=u, num_bins=num_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(a, u, pos_label=1)\n",
    "auroc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'u': u, 'a': a}).sort_values('u', ascending=False)\n",
    "df['amean'] = df['a'].expanding().mean()\n",
    "auarc = metrics.auc(np.linspace(0,1,len(df)), df['amean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 1\n",
    "\n",
    "x = np.linspace(0,1,len(df))\n",
    "mask = x <= cutoff\n",
    "\n",
    "uq_name = 'method'\n",
    "\n",
    "name_map = lambda x: x\n",
    "\n",
    "plt.plot(x, df['amean'].values[::-1], '-', label=name_map(uq_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sorted_by_conf = [x for _, x in sorted(zip(u, a))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sorted_by_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_conf = sorted(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_queries = len(a)\n",
    "tot_correct = sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = [tot_correct/num_queries]\n",
    "frac_reject_list = [0]\n",
    "\n",
    "remaining_correct = sum(a)\n",
    "\n",
    "\n",
    "for index in range(num_queries):\n",
    "    num_accepted_points = num_queries - (index + 1)\n",
    "    #print(num_accepted_points)\n",
    "    remaining_correct -= acc_sorted_by_conf[index]\n",
    "    #print(remaining_correct)\n",
    "    this_acc = remaining_correct/num_accepted_points if num_accepted_points > 0 else 1  # change accuracy\n",
    "    this_frac_reject = 1 - (num_accepted_points/num_queries)\n",
    "    \n",
    "    acc_list.append(this_acc)\n",
    "    frac_reject_list.append(this_frac_reject)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_reject_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auarc = metrics.auc(frac_reject_list, acc_list)\n",
    "print(auarc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = lambda x: x\n",
    "uq_name = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(frac_reject_list, acc_list, '--' if uq_name == 'oracle' else '-',\n",
    "                 label=name_map(uq_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['a'].expanding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"u\": u, 'a': a}).sort_values('u', ascending=True)\n",
    "df['amean'] = df['a'].expanding().mean()\n",
    "auarc = metrics.auc(np.linspace(0,1,len(df)), df['amean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[-1 * i for i in [1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['amean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 1\n",
    "\n",
    "x = np.linspace(0,1,len(df))\n",
    "mask = x <= cutoff\n",
    "\n",
    "name_map = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_name = 'method'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[mask], df['amean'].values[::-1][mask], '--' if uq_name == 'oracle' else '-',\n",
    "                 label=name_map(uq_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oracle AUARC\n",
    "\n",
    "#conf = [-1 * acc for acc in a]\n",
    "\n",
    "\n",
    "#df = pd.DataFrame({\"u\": conf, 'a': a}).sort_values('u', ascending=True)\n",
    "df = pd.DataFrame({\"u\": a, 'a': a}).sort_values('u', ascending=False)\n",
    "\n",
    "df['amean'] = df['a'].expanding().mean()\n",
    "auarc = metrics.auc(np.linspace(0,1,len(df)), df['amean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data for Fitting Conditional Similarity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "#dataset = 'MDE55'\n",
    "#temperature = '1'\n",
    "\n",
    "#dataset = 'Spider_deepseeker'\n",
    "#dataset = 'Spider_fp'\n",
    "\n",
    "\n",
    "dataset = 'Spider_codellama'\n",
    "\n",
    "sampling_type = 'temp_first'\n",
    "#sampling_type = 'temp_all'\n",
    "split = 'dev'\n",
    "#split = 'test'\n",
    "temperature = None\n",
    "\n",
    "#sampling_type = 'standard'\n",
    "#temperature = '0.25'\n",
    "#temperature = '1.0'\n",
    "\n",
    "#temperature = 0.5\n",
    "#temperature = 1\n",
    "#temperature = 3\n",
    "\n",
    "\n",
    "#dataset = 'Bird_granite'\n",
    "#temperature = None\n",
    "\n",
    "\n",
    "if dataset == 'MDE55':\n",
    "    # load MDE data\n",
    "    samples_data = evaluation.load_samples_data_MDE55(temperature)\n",
    "elif dataset == 'Spider_deepseeker':\n",
    "    # load and process Spider data for deepseeker\n",
    "    samples_data = evaluation.load_and_process_samples_data_spider_deepseeker()\n",
    "elif dataset == 'Spider_fp':\n",
    "    samples_data = evaluation.load_and_process_samples_data_spider_fp()\n",
    "elif dataset == 'Spider_codellama':\n",
    "    # load and process Spider data for codellama output\n",
    "    samples_data = evaluation.load_and_process_samples_data_spider_codellama_fewshot(sampling_type, split, temperature)\n",
    "elif dataset == 'Bird_granite':\n",
    "    # load and process Bird data for granite\n",
    "    samples_data = evaluation.load_and_process_samples_data_bird()\n",
    "else:\n",
    "    print('This dataset is not covered!')\n",
    "\n",
    "len(samples_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# inputs\n",
    "sim_type = 'jaccard'\n",
    "eps = 0.0001\n",
    "\n",
    "num_test_samples = 5\n",
    "frac_test = 0.5\n",
    "\n",
    "output_filename = 'sim_data_' + sim_type + '_' + dataset + '.json'\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "num_queries = len(samples_data)\n",
    "num_instances_test_set = int(np.floor(frac_test * num_queries))\n",
    "num_instances_valid_set = num_queries - num_instances_test_set\n",
    "\n",
    "SEED = 10\n",
    "random.seed(SEED)\n",
    "\n",
    "for index in range(num_test_samples):\n",
    "    \n",
    "    data_id = index + 1\n",
    "    \n",
    "    # split into valid/test sets\n",
    "    indices_queries = list(range(num_queries))\n",
    "    random.shuffle(indices_queries)\n",
    "    samples_data_valid = [samples_data[i] for i in indices_queries[:num_instances_valid_set]]\n",
    "    #samples_data_test = [samples_data[i] for i in indices_queries[num_instances_valid_set:]]\n",
    "    \n",
    "    acc_list_full, beta_data_given_correct, beta_data_given_incorrect, all_beta_data = \\\n",
    "    uq_from_similarity.prepare_bayes_data(samples_data_valid, sim_type, eps)\n",
    "    \n",
    "    # learn prior prob.\n",
    "    p_0 = np.mean(acc_list_full)\n",
    "    alpha_C, beta_C, alpha_I, beta_I = \\\n",
    "    uq_from_similarity.fit_sim_dist(beta_data_given_correct, beta_data_given_incorrect)\n",
    "    \n",
    "    \n",
    "    output_dict[data_id] = {\n",
    "        'sim_data_given_correct_response': beta_data_given_correct,\n",
    "        'sim_data_given_incorrect_response': beta_data_given_incorrect,\n",
    "        'beta_params_correct_response': (alpha_C, beta_C),\n",
    "        'beta_params_incorrect_response': (alpha_I, beta_I)\n",
    "    }\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "# save output\n",
    "save_bool = True\n",
    "\n",
    "if save_bool:\n",
    "    with open(output_filename, \"w\") as outfile:\n",
    "        json.dump(output_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data for Sim Dict for Some Sim Metrics\n",
    "#### aligon, aouiche, makiyama (not sbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataset = 'spider_codellama'\n",
    "\n",
    "dataset = 'spider_realistic_codellama'\n",
    "\n",
    "sim_type = 'aligon'\n",
    "save_bool = True\n",
    "\n",
    "\n",
    "sim_dict = uq_from_similarity.prep_and_save_sim_dict_ettubench(dataset, sim_type, save_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "jrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
